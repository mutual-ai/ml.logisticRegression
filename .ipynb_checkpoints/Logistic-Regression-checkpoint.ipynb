{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Definition\n",
    "Logistic regression is an algorithm for solving binary classification problems which means classifying a dataset two separate classes or groups. The case of binary classification where $y_i \\in \\{0, 1\\}$. \n",
    "The key idea is learning from a so-called feature vector $x$ to a probability, a number of between 0 and 1, where $x \\in R^n$. With logistic regression, we can predict how the probability of the y variable will changes given the changes in the x variable. Logistic regression finds the best fit S-curve that represents the points and S-curve will relate the probability of y\n",
    "\n",
    "Logistic Regression can be used with the followings:\n",
    "* Analysing consequences of past events (e.g. financial crisis, elections)\n",
    "* Allocating limited resources to opportunities (e.g. how to allocate resources to opportunities so you can maximize your benefit)\n",
    "* Predicting the outcomes of future events (e.g. credit card payments on time/late, market tomorrow up/down)\n",
    "* Classifying something into a known set of categories (e.g. emails spam/ham)\n",
    "\n",
    "Logistic regression tries to find the best fit sigmoid curve or S-curve that represents the probability of $y$ variable that is dependent to $x$ variable.\n",
    "\n",
    "The model is \n",
    "$$\n",
    "\\Pr\\{y_i = 1\\} = \\sigma(x_i^\\top w)\n",
    "$$\n",
    "\n",
    "Here, Sigmoid function is defined as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "When $x$ is given, we want to compute $P(y_i = 1 \\mid x)$ and we can represent the probability of $y$ like the one below;\n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(y) = \\frac{1}{1+e^{-x_i^\\top w}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(t):\n",
    "    return np.exp(t)/(1+np.exp(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a question that we can ask here, how can we solve this equation?\n",
    "## Maximum Likelihood Estimation\n",
    "Maximum likelihood estimation is the one of the techniques that can be used to solve the logistic regression equation. It is about that when an outcome is given, MLE tries to find the scenario in which that outcome would be most likely.\n",
    "\n",
    "Suppose we are given only $5$ outcomes when a coin is thrown:\n",
    "$$\n",
    "H, T, H, T, T\n",
    "$$\n",
    "\n",
    "What is the probabilty that the outcome is, say heads $H$ if we know that the coin is biased ?.\n",
    "\n",
    "One reasonable answer may be the frequency of heads, $2/5$.\n",
    "\n",
    "The ML solution coincides with this answer. For a derivation, \n",
    "we define $y_i$ for $i = 1,2,\\dots, 5$ as\n",
    "\n",
    "$$\n",
    "y_i  = \\left\\{ \\begin{array}{cc} 1 & \\text{coin $i$ is H} \\\\ 0 & \\text{coin $i$ is T}  \\end{array} \\right. \n",
    "$$\n",
    "hence \n",
    "$$\n",
    "y = [1,0,1,0,0]^\\top\n",
    "$$\n",
    "\n",
    "If we assume that the outcomes were independent, the probability of observing the above sequence as a function of the parameter $\\pi$ is the product of each individual probability\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Pr\\{y = [1,0,1,0,0]^\\top\\} & = & \\pi \\cdot (1-\\pi) \\cdot \\pi \\cdot (1-\\pi) \\cdot(1-\\pi) \\\\\n",
    "& = & \\pi^2 \\cdot (1 - \\pi)^3\n",
    "\\end{eqnarray}\n",
    "\n",
    "We could try finding the $\\pi$ value that maximizes this function. We will call the corresponding value as the maximum likelhood solution. \n",
    "\n",
    "It is often more convenient to work with the logarithm of this function, known as the loglikelihood function.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\pi) = 2 \\ln \\pi + 3 \\ln (1-\\pi)\n",
    "$$\n",
    "For finding the maximum, we take the derivative with respect to $\\pi$ and set to zero.\n",
    "$$\n",
    "\\frac{d \\mathcal{L}(\\pi)}{d \\pi} = \\frac{2}{\\pi} -  \\frac{3}{1-\\pi} = 0 \n",
    "$$\n",
    "When we solve we obtain (as a maximum likelihood solution) $$ \\pi = \\frac{2}{5} $$\n",
    "\n",
    "More generally, when we observe $y_i$ for $i=1 \\dots N$, the loglikelihood is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(\\pi)& = & \\ln \\left(\\prod_{i : y_i=1} \\pi \\cdot \\prod_{i : y_i=0}(1 - \\pi) \\right) \\\\\n",
    "& = & \\ln \\left(\\prod_{i = 1}^N \\pi^{y_i} \\cdot (1- \\pi)^{1-y_i} \\right) \\\\\n",
    "& = & \\ln  \\left(\\pi^{ \\sum_i y_i} \\cdot (1- \\pi)^{\\sum_i (1-y_i) } \\right) \\\\\n",
    "& = & \\left(\\sum_i y_i\\right) \\ln \\pi + \\left(\\sum_i (1-y_i) \\right) \\ln (1- \\pi) \n",
    "\\end{eqnarray}\n",
    "\n",
    "If we define the number of observed $0$'s and $1$'s by $c_0$ and $c_1$ respectively, we have \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(\\pi)& = & c_1 \\ln \\pi + c_0 \\ln (1- \\pi) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Taking the derivative and setting to $0$ results in\n",
    "\n",
    "$$\n",
    "\\pi = \\frac{c_1}{c_0+c_1} = \\frac{c_1}{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logsumexp(a,b):\n",
    "    m = np.max([a,b])\n",
    "    return m + np.log(np.exp(a - m) + np.exp(b - m))\n",
    "\n",
    "def hinge(x):\n",
    "    return x if x>=0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the parameters\n",
    "\n",
    "The logistic regression model is very similar to the coin model. The main difference is that we use a specific coin with a probability $\\sigma(x_i^\\top w)$ that depends on the specific feature vector and the global parameter vector $w$.  \n",
    "The likelihood of the observations, that is the probability of observing the class sequence is\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "p(y_1, y_2, \\dots, y_N|w, X ) &=& \\left(\\prod_{i : y_i=1} \\sigma(x_i^\\top w) \\right) \\left(\\prod_{i : y_i=0}(1- \\sigma(x_i^\\top w)) \\right)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "Here, the left product is the expression for examples from class $1$ and the right product is for examples from class $0$.\n",
    "We will look for the particular setting of the weight vector, the maximum likelihood solution, denoted by $w^*$.\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "w^* & = & \\arg\\max_{w} {\\cal L}(w)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "where the loglikelihood function\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\ln p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) \\\\\n",
    "& = & \\sum_{i : y_i=1} \\ln \\sigma(x_i^\\top w) + \\sum_{i : y_i=0} \\ln (1- \\sigma(x_i^\\top w)) \\\\\n",
    "& = & \\sum_{i : y_i=1} x_i^\\top w - \\sum_{i : y_i=1} \\ln(1+e^{x_i^\\top w}) - \\sum_{i : y_i=0}\\ln({1+e^{x_i^\\top w}}) \\\\\n",
    "& = & \\sum_i y_i x_i^\\top w - \\sum_{i} \\ln(1+e^{x_i^\\top w}) \\\\\n",
    "& = & y^\\top X w - \\mathbf{1}^\\top \\text{logsumexp}(0, X w)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "$\\mathbf{1}$ is a vector of ones; note that when we premultiply a vector $v$ by $\\mathbf{1}^T$ we get the sum of the entries of $v$, i.e. $\\mathbf{1}^T v = \\sum_i v_i$.\n",
    "\n",
    "We define the function $\\text{logsumexp}(a, b)$ as follows: When $a$ and $b$ are scalars, \n",
    "$$\n",
    "f = \\text{logsumexp}(a, b) \\equiv \\ln(e^a + e^b)\n",
    "$$\n",
    "\n",
    "When $a$ and $b$ are vectors of the same size, $f$ is the same size as $a$ and $b$ where each entry of $f$ is\n",
    "$$\n",
    "f_i = \\text{logsumexp}(a_i, b_i) \\equiv \\ln(e^{a_i} + e^{b_i})\n",
    "$$\n",
    "\n",
    "Unlike the least-squares problem, an expression for direct evaluation of $w^*$ is not known so we need to resort to numerical optimization. \n",
    "\n",
    "Before we proceed, it is informative to look at the shape of $f(x) = \\text{logsumexp}(0, x)$.\n",
    "When $x$ is negative and far smaller than zero, $f = 0$ and for large values of $x$, $f(x) = x$. Hence it looks like a so-called hinge function $h$\n",
    "$$\n",
    "h(x) = \\left\\{ \\begin{array}{cc} 0 & x < 0 \\\\x & x \\geq 0  \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "We define\n",
    "$$\n",
    "f_\\alpha(x) = \\frac{1}{\\alpha}\\text{logsumexp}(0, \\alpha x)\n",
    "$$\n",
    "When $\\alpha = 1$, we have the original logsumexp function. For larger $\\alpha$, it becomes closer to the hinge loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization via gradient ascent\n",
    "\n",
    "One way for\n",
    "optimization is gradient ascent\n",
    "\\begin{eqnarray}\n",
    "w_{\\tau} & \\leftarrow & w_{\\tau-1} + \\eta \\nabla_w {\\cal L}\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\nabla_w {\\cal L} & = &\n",
    "\\begin{pmatrix}\n",
    "{\\partial {\\cal L}}/{\\partial w_1} \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_{D}}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "is the gradient vector and $\\eta$ is a learning rate.\n",
    "\n",
    "#### Evaluating the gradient (Short Derivation)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = y^\\top X w - \\mathbf{1}^\\top \\text{logsumexp}(0, X w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}(w)}{dw} = X^\\top y - X^\\top \\sigma(X w) = X^\\top (y -\\sigma(X w))\n",
    "$$\n",
    "\n",
    "#### Evaluating the gradient (Long Derivation)\n",
    "The partial derivative of the loglikelihood with respect to the $k$'th entry of the weight vector is given by the chain rule as\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\frac{\\partial{\\cal L}}{\\partial \\sigma(u)} \\frac{\\partial \\sigma(u)}{\\partial u} \\frac{\\partial u}{\\partial w_k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\sum_{i : y_i=1} \\ln \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\ln (1- \\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}(\\sigma)}{\\partial \\sigma} & = &  \\sum_{i : y_i=1} \\frac{1}{\\sigma(w^\\top x_i)} - \\sum_{i : y_i=0} \\frac{1}{1- \\sigma(w^\\top x_i)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\sigma(u)}{\\partial u} & = & \\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w^\\top x_i }{\\partial w_k} & = & x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "So the gradient is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{\\sigma(w^\\top x_i)} x_{i,k} - \\sum_{i : y_i=0} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{1- \\sigma(w^\\top x_i)} x_{i,k} \\\\\n",
    "& = & \\sum_{i : y_i=1} {(1-\\sigma(w^\\top x_i))} x_{i,k} - \\sum_{i : y_i=0} {\\sigma(w^\\top x_i)} x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can write this expression more compactly by noting\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} {(\\underbrace{1}_{y_i}-\\sigma(w^\\top x_i))} x_{i,k} + \\sum_{i : y_i=0} {(\\underbrace{0}_{y_i} - \\sigma(w^\\top x_i))} x_{i,k} \\\\\n",
    "& = & \\sum_i (y_i - \\sigma(w^\\top x_i)) x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "$\\newcommand{\\diag}{\\text{diag}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientAscentStep(eta, X, y, w):\n",
    "    dL = np.dot(X.T, y - sigmoid(np.dot(X, w)))\n",
    "    w = w + eta * dL\n",
    "    return w\n",
    "# end of def gradientAscentStep\n",
    "\n",
    "def gradientAscent(eta, X, y, w, epoch):\n",
    "    for i in range(epoch):\n",
    "        w = gradientAscentStep(eta, X, y, w)\n",
    "    return w\n",
    "# end of def gradientAscent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sl   sw   pl   pw  c\n",
      "0   5.1  3.5  1.4  0.2  1\n",
      "6   4.6  3.4  1.4  0.3  1\n",
      "12  4.8  3.0  1.4  0.1  1\n",
      "18  5.7  3.8  1.7  0.3  1\n",
      "24  4.8  3.4  1.9  0.2  1\n",
      "    sl   sw   pl   pw  c\n",
      "1  4.9  3.0  1.4  0.2  1\n",
      "2  4.7  3.2  1.3  0.2  1\n",
      "3  4.6  3.1  1.5  0.2  1\n",
      "4  5.0  3.6  1.4  0.2  1\n",
      "5  5.4  3.9  1.7  0.4  1\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "dataset = pd.read_csv('iris.txt', sep=' ')\n",
    "\n",
    "test_indis = 6\n",
    "\n",
    "test_dataset = dataset[dataset.index % test_indis == 0]\n",
    "print(test_dataset.head())\n",
    "\n",
    "train_dataset = dataset[dataset.index % test_indis != 0]\n",
    "print(train_dataset.head())\n",
    "\n",
    "# total count of sample space\n",
    "total = float(len(train_dataset))\n",
    "\n",
    "columns = [\"sl\",\"sw\", \"pl\", \"pw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 1.55358575  4.90640474 -7.91223125 -3.57262903]\n",
      "y: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s: [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def train(eta, w):\n",
    "    X = np.array(train_dataset[columns])\n",
    "    y = [1 if v == 1 else 0 for v in np.array(train_dataset[[\"c\"]])]\n",
    "    w = gradientAscent(eta, X, y, w, 1000)\n",
    "    return w\n",
    "# end of def train\n",
    "\n",
    "def test(w):\n",
    "    X = np.array(test_dataset[columns])\n",
    "    y = [1 if v == 1 else 0 for v in np.array(test_dataset[[\"c\"]])]\n",
    "    print(\"y:\",y)\n",
    "    Z = np.dot(X, w)\n",
    "    Z_dist = np.array([1 if v >= 0.5 else 0 for v in sigmoid(Z)])\n",
    "    print(\"s:\", Z_dist)\n",
    "    \n",
    "    # compute accuracy\n",
    "    correct = (y == Z_dist).sum()\n",
    "    \n",
    "    print(\"accuracy:\", correct / len(y))\n",
    "# end of def test\n",
    "\n",
    "# intial values\n",
    "w = np.array(np.zeros(4))\n",
    "\n",
    "eta = 0.02\n",
    "\n",
    "w = train(eta, w)\n",
    "print(\"w:\", w)\n",
    "test(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation by using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s: [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# improt pytorch libraries\n",
    "import torch as py\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# determine learning rate\n",
    "eta = 0.02\n",
    "\n",
    "# create a linear model. y = w*x + b\n",
    "model = py.nn.Linear(4, 1)\n",
    "\n",
    "# define an optimizer. I have defined Stochastic Gradient Descent.\n",
    "optim = py.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "# define loss function. It is Binary Cross Entropy as a loss function.\n",
    "loss = py.nn.BCELoss(size_average=True)\n",
    "\n",
    "# definition of sigmoid funtion to normalize calculation to between 0 and 1 for BCELoss.\n",
    "sigmoid = py.nn.Sigmoid()\n",
    "\n",
    "def gradient(X, y, epoch):\n",
    "    # gradient steps\n",
    "    for i in range(epoch):\n",
    "        # compute loss value.\n",
    "        dE = loss(sigmoid(model(Variable(X))) , Variable(y))\n",
    "        # reset of gradient buffer\n",
    "        optim.zero_grad()\n",
    "        # compute gradients with respect to vectors of X by using auto differitiation.\n",
    "        dE.backward()\n",
    "        # update optimizer.\n",
    "        optim.step()\n",
    "# end of def gradient\n",
    "\n",
    "def train():\n",
    "    # get train dataset\n",
    "    X = py.from_numpy(np.array(train_dataset[columns])).float()\n",
    "    # get classes from train dataset\n",
    "    y = py.from_numpy(np.array([[1] if v == 1 else [0] for v in np.array(train_dataset[[\"c\"]])])).float()\n",
    "    # train them.\n",
    "    gradient(X, y, 100)\n",
    "# end of def train\n",
    "\n",
    "def test():\n",
    "    # get test_dataset\n",
    "    X = py.from_numpy(np.array(test_dataset[columns])).float()\n",
    "    # get classes from test dataset.\n",
    "    y = [1 if v == 1 else 0 for v in np.array(test_dataset[[\"c\"]])]\n",
    "    print(\"y:\",y)\n",
    "    # classify them\n",
    "    Z = sigmoid(model(Variable(X)))\n",
    "    # get classification results.\n",
    "    Z_dist = np.array([1 if (v.data >= 0.5).numpy() else 0 for v in Z])\n",
    "    print(\"s:\", Z_dist)\n",
    "    \n",
    "    # compute accuracy.\n",
    "    correct = (y == Z_dist).sum()\n",
    "    \n",
    "    print(\"accuracy:\", correct / len(y))\n",
    "# end of def test\n",
    "\n",
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear regression tries to understand and model continuous variables. With linear regression, we can predict how a y variable will change because of changes in th x variable. Lİnear regression finds best fit line that represents the points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [github notes](https://github.com/atcemgil/notes/blob/master/LogisticRegression.ipynb), Ali Taylan Cemgil, Bogaziçi University, İstanbul, Turkey\n",
    "* [coursera](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/LoKih/logistic-regression), Neural Networks and Deep Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
